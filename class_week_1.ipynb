{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.69:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Computing with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is distributed computing?\n",
    "\n",
    "An approach used for processing large volumes of data\n",
    "\n",
    "**Main components:**\n",
    "- **Cluster** - a collection of systems that work together to perform functions\n",
    "- **Node** - individual servers within a cluster\n",
    "\n",
    "**Principles:**\n",
    "- **\"Scale out\"** instead of ~~\"scale up\"~~:\n",
    "    - *Cheaper* : run operations on clusters of smaller and cheaper machine\n",
    "    - *Reliable* (Fault tolerant): in case of a node failure, its work could be assumed by other components in a system\n",
    "    - *Faster*: parallelization and distribution of computations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Apache Spark?\n",
    "\n",
    "It's an open-source cluster computing framework for real-time processing. Spark provides an interface for programming clusters with the implicit  parallelism and resilience (fault-tolerance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map-Reduce Paradigm\n",
    "\n",
    "Allows computations to be parallelized over a cluster. \n",
    "In a **basic form** it allows:\n",
    " -  to plan **map** tasks to be run on the correct nodes and shuffle data for the **reduce** operation\n",
    " - ** map**: apply a function to each key-value pair over a portion of data in parallel. E.g.: filter()\n",
    " - **reduce**: return one key-value pair from multiple key-value pairs. E.g.: sum(), sount()\n",
    "\n",
    "**Example**\n",
    "\n",
    "Walmart orders:\n",
    "\n",
    "**Step1** Orders distributed b/n different nodes:\n",
    "\n",
    "||Order number| Node number| Item| Operation\n",
    "|:------------:|:------------|-----|:-----------\n",
    "|1|A|M|{ProductName: SF Giants Hat, {Qty:1, UnitPrice:10, Price:10}}\n",
    "|2|B|M|{ProductName: SF Giants Hat, {Qty:2, UnitPrice:10, Price:20}}\n",
    "|2|B|S|{ProductName: SF Giants Hat, {Qty:3, UnitPrice:8, Price:24}}\n",
    "\n",
    "**Step2:**\n",
    "\n",
    " ** MAP **:\n",
    " Put only the quanity and Price information\n",
    " \n",
    "|| Node number| Item| Operation\n",
    "|:------------:|:------------|-----|:-----------\n",
    "|A|M|{ProductName: SF Giants Hat, {Qty:1, Price:10}}\n",
    "|B|M|{ProductName: SF Giants Hat, {Qty:2, Price:20}}\n",
    "|B|S|{ProductName: SF Giants Hat, {Qty:3, Price:24}}\n",
    "\n",
    "**Step3**\n",
    "\n",
    "**REDUCE**:\n",
    "For each item compute total price paid by a customer (this would condense M hats)\n",
    "\n",
    "|| Node number| Item| Operation\n",
    "|:------------:|:------------|-----|:-----------\n",
    "|A|M|{ProductName: SF Giants Hat, {Qty:3, Price:30}}\n",
    "|B|S|{ProductName: SF Giants Hat, {Qty:3, Price:24}}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop MapReduce\n",
    "\n",
    "An open-source distributed Java computation framework.\n",
    "Consists of:\n",
    "\n",
    "- Hadoop Common\n",
    "- Hadoop Distributed File aSystem (HDFS)\n",
    "- YARN\n",
    "- MapReduce\n",
    "\n",
    "Solves the issues:\n",
    "- Parallelism\n",
    "- Distribution\n",
    "- Fault Tolerance\n",
    "\n",
    "\n",
    "However,Hadoop MR has a number of drawbacks:\n",
    "\n",
    "- Can be slow: MR results are stored on a disk () before thy used in another job ==> very slow with iterative algorithms\n",
    "- Many types of problems don't fit MR's two-step paradigm\n",
    "- A low-level framework which gave rise to millions of tools built on top of it -->> increased complexity and requirements\n",
    "\n",
    "![](https://www.packtpub.com/sites/default/files/Article-Images/B05195_4.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop MapReduce vs Spark\n",
    "\n",
    "|  |Hadoop|Spark\n",
    "|-----|-----|----\n",
    "|**Speed**|Decrntly fast| 100 times faster than Hadoop\n",
    "|**Ease of use**| No intercative modes and hard to learn | there are interactive modes; easy to learn\n",
    "|**Costs**|Open-source| Open-source\n",
    "|**Data processing**| In batches |Bach processing + Streaming\n",
    "|**Fault Tolerance**| Yes|Yes\n",
    "|**Security**| Kerberos auth| Password auth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why Spark is faster?\n",
    "\n",
    "- It keeps large amount of data in **memory** (hence, x100 faster than Hadoop) => good for iterative algorithms (ML, graphs and others that need to reuse data)\n",
    "- You can write distributed programs in a manner similar to writing local programs, b/c Spark abstracts away the fact that programs are referencing data distributed on a large number of nodes\n",
    "- Spark combines Hadoop MR - like capabilitues, real-time processing, SQL- like handling of structured data, graph algo-s and ML in a single framework\n",
    "- Spark extends MR mdoel with primitives for efficient data sharing using RDD "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Feature|Details\n",
    "|-------|-------\n",
    "|Speed| x100 faster than Hadoop in memory, or x10 faster on disk; DAG execution engine, in-memory computing\n",
    "|Speed of use| <ul><li>Supports many lang-s: Scala, Python, Java, R </li><li>offers >80 high-level operations to make build parallel apps esier</li> <li>Supports interactive mode (e.g. Pyspark) </li></ul>\n",
    "|Generality|  Combines <ul><li>Spark SQL</li><li>Spark Streaming</li><li> MLib (machine learning)</li> <li>GraphX</li></ul>\n",
    "|Code length| Same operations could be programmed with x3 less lines of code\n",
    "|Runs everywhere| <ul><li>Hadoop</li><li>Mesos</li><li>Standalone</li><li>in the cloud</li></ul>\n",
    "|Access|<ul><li>Cassandra</li><li>HDFS</li><li>HBase</li><li>Hive</li><li>...</li></ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Stack\n",
    "\n",
    "**Cluster Managers**:\n",
    "- standalone\n",
    "- YARN\n",
    "- Mesos\n",
    "\n",
    "**Spark Core**\n",
    "\n",
    "**Frameworks**\n",
    "- Spark SQL\n",
    "- Spark ML/MLlib\n",
    "- Spark Streaming\n",
    "- GraphX\n",
    "\n",
    "![](https://www.safaribooksonline.com/library/view/learning-spark/9781449359034/assets/lnsp_0101.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Spark Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Spark Core**: Contains Spark functionalities requried for running jobs and needed by other components \n",
    "    - **RDD (Resilient Distributed Dataset**: abstraction of distributed collection of items with operations and transformation applicable to the dataset.\n",
    "    - **Fundamental functions**: networking, security, scheduling and data shuffling\n",
    "    - **Logic**: connects RDDs to underlying distributed file system, such as S3, HDFS, GlusterFS\n",
    "\n",
    "\n",
    "2. **Spark SQL**:\n",
    "    - Functions for manipulationg large sets of distributed, structured data\n",
    "    - Uses ```DataFrames``` and ```DataSets```: Spark SQL transforms oeprations on DF's and DS's to operations on RDD\n",
    "    - Data sources: Hive, JSON, relational DB's, NoSQL databases and Parquet files\n",
    "\n",
    "3. **Spark Streamimg**:\n",
    "    - Ingest real-time data straming from various redources: HDFS, Kafka, Twitter, ZeroHQ ...\n",
    "    - Automatic recover from failure\n",
    "    - represent streaming data using discretized streams (Dstreams) which periodically create RDDs containing the data that came in during the last time window.\n",
    "    - Can be combined with other Spark components\n",
    "\n",
    "4. **Spark MLlib and ML**\n",
    "    - Library of machine kearning algo-s; includes:\n",
    "        - logistic/linear regression;\n",
    "        - naive Bayes\n",
    "        - SVM\n",
    "        - decision trees\n",
    "        - RF\n",
    "        - k-means clusttering\n",
    "        \n",
    "    - MLlib: RDD_based API\n",
    "    - Spark ML: DataFrame-based API\n",
    "\n",
    "5. **Spark GraphX** \n",
    "    - Provide functions for building graphs represented as graph RDDs: ```EdgeRDD``` and ```VertexRDD```\n",
    "    - Contains important algorithms of graph theory s.a. page rank, connected components, shortest paths...\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Examples\n",
    "- Extract-transformation-load (ETL) operations\n",
    "- Predictive analytics\n",
    "- Machine learning\n",
    "- Data access operation (SQL queries and visualizations) \n",
    "- Text mining and text processing\n",
    "- Real-time event processing \n",
    "- Graph applications\n",
    "- Pattern Recognition \n",
    "- Recommendation engines\n",
    "- And many more.. \n",
    "\n",
    "Source: http://spark.apache.org/examples.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### How to install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
